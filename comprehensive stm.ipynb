{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive STM Workflow\n",
    "1. STM run with k=0\n",
    "1. NbClustering run over prelim STMs' theta to find better number of topics\n",
    "1. STM viewer webapp STM json data outputted\n",
    "1. Manual STM refinement / tweaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(magrittr)\n",
    "library(stm)\n",
    "library(jsonlite)\n",
    "library(doMC)\n",
    "library(foreach)\n",
    "library(NbClust)\n",
    "library(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load('../data_processing/tidy_questions.Rda')\n",
    "source('stmjson.R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of cores to use on following computations\n",
    "registerDoMC(cores=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary STM\n",
    "Used to find baseline topic number using STM library methods (K=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STM does not produce meaningful clusters for these questions and are best removed.\n",
    "questions %>% names %>% as.data.frame %>% slice(4:6)\n",
    "questions <- questions[-c(4,5,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start <- Sys.time()\n",
    "verbosity <- FALSE\n",
    "\n",
    "procs <- foreach(n = seq(length(questions))) %dopar% textProcessor(documents = questions[[n]][[1]],\n",
    "                                                                  metadata = questions[[n]][2],\n",
    "                                                                  customstopwords = c('art','arts'),\n",
    "                                                                  verbose = verbosity)\n",
    "\n",
    "docs <- foreach(n = seq(length(questions))) %dopar% prepDocuments(documents = procs[[n]]$documents, \n",
    "                                                                 vocab = procs[[n]]$vocab, meta = procs[[n]]$meta,\n",
    "                                                                 lower.thresh = ifelse(procs[[n]]$documents %>%\n",
    "                                                                                       length > 1000, 4, 3),\n",
    "                                                                 verbose = verbosity)\n",
    "\n",
    "prelim_stms <- foreach(n = seq(length(questions))) %dopar% stm(documents = docs[[n]]$documents,\n",
    "                                                               vocab = docs[[n]]$vocab, K = 0, \n",
    "                                                               data = docs[[n]]$meta, verbose = verbosity)\n",
    "\n",
    "time_taken <- Sys.time() - start\n",
    "time_taken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NbClust Cluster Analysis\n",
    "Used to find very close estimates of the best number of topics for each question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcas <- foreach(n = seq(length(prelim_stms))) %dopar% prcomp(x = (prelim_stms[[n]]$theta), scale. = T)\n",
    "\n",
    "nbcs <- foreach(n = seq(length(pcas))) %dopar% NbClust(data = select(data.frame(pca$x),\n",
    "                                                                     1:(stmobj$settings$dim$K - 5)),\n",
    "                                                       diss = daisy(pca$x),\n",
    "                                                       distance=NULL,\n",
    "                                                       min.nc=3,\n",
    "                                                       max.nc=27,\n",
    "                                                       method='complete',\n",
    "                                                       index='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The methods filtered out seem to always choose the lowest number of clusters considered every time.\n",
    "k_canidates <- c()\n",
    "for(i in seq(nbcs)) {\n",
    "    num_clust <- data.frame(method=nbcs[[i]]$Best.nc %>% t %>% rownames,\n",
    "               nc=nbcs[[i]]$Best.nc %>% t %>% as.data.frame() %>% pull(1)) %>% \n",
    "                    filter(method != 'Cindex' & method != 'DB' & method != 'Silhouette' &\n",
    "                           method != 'Duda' & method != 'PseudoT2' & method != 'Beale' &\n",
    "                           method != 'McClain' & method != 'Hubert' & method != 'Dindex')\n",
    "    num_clust %<>% pull(2) %>% table %>% data.frame %>% arrange(-Freq) %>% slice(1) %>% pull(1)\n",
    "    k_canidates %<>% c(num_clust)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved STMs\n",
    "Using NbClust recommended numbers of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start <- Sys.time()\n",
    "verbosity <- FALSE\n",
    "\n",
    "improved_stms <- foreach(n = seq(length(questions))) %dopar% stm(documents = docs[[n]]$documents,\n",
    "                                                                 vocab = docs[[n]]$vocab, K = k_canidates[n],\n",
    "                                                                 data = docs[[n]]$meta, verbose = verbosity)\n",
    "\n",
    "time_taken <- Sys.time() - start\n",
    "time_taken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputting STM Data\n",
    "To be used with the webapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_names <- c()\n",
    "for(i in seq(questions)) {\n",
    "    question_names %<>% c(names(questions[[i]][1]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './'\n",
    "\n",
    "foreach(n = seq(length(questions))) %dopar% create_json(\n",
    "    stm = improved_stms[[n]],\n",
    "    documents_raw = questions[[n]][question_names[n]] %>% slice(-procs[[n]]$docs.removed) %>% \n",
    "                                                                       slice(-docs[[n]]$docs.removed) %>% \n",
    "                                                                       pull,\n",
    "    documents_matrix = docs[[n]]$documents,\n",
    "    column_name = question_names[[n]],\n",
    "    title = names(questions[n]),\n",
    "    clustering_thresh = 1.4, #should be as low as possible without errors (raise in 0.1 steps if errors)\n",
    "    verbose = T,\n",
    "    directory = directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STM Refinement\n",
    "Use this space to change the number of topics, lower.thresh, and stopwords of questions to try to make a qualitatively better model after having inspecting/comparing the model in the STM viewer webapp. A good place to start is looking at how well defined the no/none topic is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#/ use this space to write down the question number, best number of topics, and custom stop words \n",
    "c(15, 9, )\n",
    "c(16, 11, c('art','arts','grow','growth','develop','development','way'))\n",
    "c(17, 13, c('art','arts','positive','negative','helped','major','really',\n",
    "              'much','made','think','dont','don\\'t','experience','experiences',\n",
    "              'college','most','life','role','provided')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i <- 19\n",
    "ntopics <- 11\n",
    "\n",
    "procs[[i]] <- textProcessor(documents = questions[[i]][[1]], \n",
    "              metadata = questions[[i]][2],\n",
    "              customstopwords = c('art','arts','grow','growth','develop','development','way'))\n",
    "#               customstopwords = c('art','arts','positive','negative','helped','major','really',\n",
    "#                                   'much','made','think','dont','don\\'t','experience','experiences',\n",
    "#                                   'college','most','life','role','provided'))\n",
    "#               customstopwords = c('art','arts'))\n",
    "\n",
    "docs[[i]] <- prepDocuments(documents = procs[[i]]$documents,\n",
    "              vocab = procs[[i]]$vocab,\n",
    "              meta = procs[[i]]$meta,\n",
    "              lower.thresh = 3)\n",
    "              #lower.thresh = ifelse(procs[[i]]$documents %>% length > 1000, 4, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start <- Sys.time()\n",
    "stmobj <- stm(documents = docs[[i]]$documents,\n",
    "                vocab = docs[[i]]$vocab,\n",
    "                K = ntopics,\n",
    "                data=docs[[i]]$meta,\n",
    "                verbose=F)\n",
    "print(Sys.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment out one of the following lines\n",
    "labels <- read_json('labels/sr_othergrowth_labels.json')\n",
    "# labels <- NULL\n",
    "\n",
    "if (is.null(labels)) {\n",
    "    labels$topics <- NULL\n",
    "    labels$clusters <- NULL\n",
    "}\n",
    "\n",
    "create_json(\n",
    "    stm = stmobj,\n",
    "    documents_raw = questions[[i]][question_names[i]] %>% slice(-procs[[i]]$docs.removed) %>% \n",
    "                                                                       slice(-docs[[i]]$docs.removed) %>% \n",
    "                                                                       pull,\n",
    "    documents_matrix = docs[[i]]$documents,\n",
    "    column_name = question_names[[i]],\n",
    "    title = names(questions[i]),\n",
    "    clustering_thresh = 1.4, # should be as low as possible w/o errors\n",
    "    instant = T, # names the json data.json\n",
    "    topic_labels = labels$topics,\n",
    "    cluster_labels = labels$clusters,\n",
    "    directory = './'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
